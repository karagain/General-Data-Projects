# For Reddit Sub Subs
For this project, I scraped the names of the cities and states off wikipedia page for the largest cities per state using the package Beautiful Soup, used the names of the cities as inputs added on to the ends of urls to access the subreddits, and then collected the information from each subreddit for visualization. The results show the subscriber counts for major cities but is flawed because some areas are high in subscriber counts but not large in size, for example San Francisco. Another flaw is that some locations use the area for the subreddit instead of the city name, for that reason I have also omitted those variables to explicitly look only at subscribers based on cities. This project demonstrates my ability to produce projects related to companies as well as scraping the results according to the API. An example of this would be the sleep timer that would run every 10 seconds, or the use of the API for simpler collection. This project also shows the raw workflow for data cleaning, editing, and re-editing after visualization. 

This is a cleaning focused, and visualization data project. 

# Motivation
This purpose of this project was to visualize a metric of performance for Reddit that could be improved on through growth. It was also meant to be done quickly showing as much of the workflow as possible to simulate what I could be doing in a work environment. 

# Build Status
Complete

# Software
Tableau Public    
Jupyter Notebook

# Credits
